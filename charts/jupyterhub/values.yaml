# fullnameOverride and nameOverride distinguishes blank strings, null values,
# and non-blank strings. For more details, see the configuration reference.
fullnameOverride: ""
nameOverride:

# custom can contain anything you want to pass to the hub pod, as all passed
# Helm template values will be made available there.
custom: { }

# imagePullSecret is configuration to create a k8s Secret that Helm chart's pods
# can get credentials from to pull their images.
imagePullSecret:
  create: false
  automaticReferenceInjection: true
  registry:
  username:
  password:
  email:
# imagePullSecrets is configuration to reference the k8s Secret resources the
# Helm chart's pods can get credentials from to pull their images.
imagePullSecrets: [ ]

# hub relates to the hub pod, responsible for running JupyterHub, its configured
# Authenticator class KubeSpawner, and its configured Proxy class
# ConfigurableHTTPProxy. KubeSpawner creates the user pods, and
# ConfigurableHTTPProxy speaks with the actual ConfigurableHTTPProxy server in
# the proxy pod.
hub:
  config:
    JupyterHub:
      admin_access: true
#      authenticator_class: generic-oauth
      authenticator_class: ldapauthenticator.LDAPAuthenticator
    LDAPAuthenticator:
      allowed_groups:
        - cn=cqai,ou=groups,dc=cq
        - cn=cqanalyst,ou=groups,dc=cq
        - cn=ops,ou=groups,dc=cq
      bind_dn_template:
        - uid={username},ou=users,dc=cq
      escape_userdn: false
      lookup_dn: true
      lookup_dn_search_filter: ({login_attr}={login})
      lookup_dn_user_dn_attribute: uid
      server_address: cqi.cq
      user_attribute: uid
      user_search_base: ou=users,dc=cq
#    GenericOAuthenticator:
#      client_id: dkuleshov-hub
#      client_secret: your-client-secret
#      oauth_callback_url: https://dkuleshov-hub.dev.cloudquant.ai/hub/oauth_callback
#      authorize_url: https://dex.dev.cloudquant.ai/auth
#      token_url: https://dex.dev.cloudquant.ai/token
#      userdata_url: https://dex.dev.cloudquant.ai/userinfo
#      scope:
#        - openid
#        - name
#        - profile
#        - email
#      username_key: name
    Authenticator:
      admin_users:
        - dkuleshov
    #    DummyAuthenticator:
    #      password: a-shared-secret-password

  service:
    type: ClusterIP
    annotations: { }
    ports:
      nodePort:
    extraPorts: [ ]
    loadBalancerIP:
  baseUrl: /
  cookieSecret:
  initContainers: [ ]
  fsGid: 1000
  nodeSelector: { }
  tolerations: [ ]
  concurrentSpawnLimit: 64
  consecutiveFailureLimit: 5
  activeServerLimit:
  deploymentStrategy:
    ## type: Recreate
    ## - sqlite-pvc backed hubs require the Recreate deployment strategy as a
    ##   typical PVC storage can only be bound to one pod at the time.
    ## - JupyterHub isn't designed to support being run in parallell. More work
    ##   needs to be done in JupyterHub itself for a fully highly available (HA)
    ##   deployment of JupyterHub on k8s is to be possible.
    type: Recreate
  db:
    type: sqlite-pvc
    upgrade:
    pvc:
      annotations: { }
      selector: { }
      accessModes:
        - ReadWriteOnce
      storage: 1Gi
      subPath:
      storageClassName:
    url:
    password:
  labels: { }
  annotations: { }
  command: [ ]
  args: [ ]
  extraConfig:
    legacy: |
      c.JupyterHub.logo_file = '/usr/local/share/jupyter/hub/static/images/logo.png'
      c.JupyterHub.ip = '127.0.0.1'
      c.Spawner.args = [f'--NotebookApp.allow_origin={"*"}']
      c.JupyterHub.tornado_settings = {'headers': {'Access-Control-Allow-Origin': '*',},}
      c.NotebookApp.allow_origin = '*'
    uidGid: |
      from ldap3 import Server, Connection

      def ldap_get_gids(spawner):
          '''
          Return group IDs from LDAP for the user,
          '''
          groups = dict()
          server = Server(c.LDAPAuthenticator.server_address)
          conn = Connection(server, auto_bind=True)
          user = spawner.user.name
          conn.search('ou=groups,dc=cq', f'(memberUid={user})', attributes=['cn', 'gidNumber'])
          for entry in conn.entries:
              groups.update({entry.cn.value: entry.gidNumber.value})
          spawner.environment['LDAP_GROUPS'] = ':'.join(
            [f'{group}={gid}' for (gid, group)
            in sorted(groups.items(), key=lambda x: x[1])  # sort on num
            ])
          spawner.environment['NB_USER'] = user
          spawner.environment['NB_USERNAME'] = user
          spawner.environment['NB_GID'] = user
          gids = list(groups.values())
          if 1000 not in gids:
            gids.append(1000)
            gids.append(100)
          return gids

      def ldap_get_uid(spawner):
          '''
          Return user ID from LDAP for the user,
          '''
          server = Server(c.LDAPAuthenticator.server_address)
          conn = Connection(server, auto_bind=True)
          user = spawner.user.name
          conn.search('ou=users,dc=cq', f'(&(objectClass=person)(uid={user}))', attributes=['uidNumber'])
          uid = conn.entries[0]['uidNumber']

          return uid.value

      c.KubeSpawner.uid = ldap_get_uid
      c.KubeSpawner.gid = 100
      c.KubeSpawner.fs_gid = ldap_get_uid
      c.KubeSpawner.supplemental_gids = ldap_get_gids
  extraFiles: { }
  extraEnv: { }
  extraContainers: [ ]
  extraVolumes:
    - name: templates
      secret:
        secretName: hub-templates
  extraVolumeMounts:
    - name: templates
      mountPath: /srv/templates
  image:
    name: jupyterhub/k8s-hub
    tag: "1.2.0"
    pullPolicy:
    pullSecrets: [ ]
  resources: { }
  containerSecurityContext:
    runAsUser: 1000
    runAsGroup: 1000
    allowPrivilegeEscalation: false
  lifecycle: { }
  services: { }
  pdb:
    enabled: false
    maxUnavailable:
    minAvailable: 1
  networkPolicy:
    enabled: true
    ingress: [ ]
    ## egress for JupyterHub already includes Kubernetes internal DNS and
    ## access to the proxy, but can be restricted further, but ensure to allow
    ## access to the Kubernetes API server that couldn't be pinned ahead of
    ## time.
    ##
    ## ref: https://stackoverflow.com/a/59016417/2220152
    egress:
      - to:
          - ipBlock:
              cidr: 0.0.0.0/0
    interNamespaceAccessLabels: ignore
    allowedIngressPorts: [ ]
  allowNamedServers: false
  namedServerLimitPerUser:
  authenticatePrometheus:
  redirectToServer:
  shutdownOnLogout:
  templatePaths: ['/srv/templates']
  templateVars: { }
  livenessProbe:
    # The livenessProbe's aim to give JupyterHub sufficient time to startup but
    # be able to restart if it becomes unresponsive for ~5 min.
    enabled: true
    initialDelaySeconds: 300
    periodSeconds: 10
    failureThreshold: 30
    timeoutSeconds: 3
  readinessProbe:
    # The readinessProbe's aim is to provide a successful startup indication,
    # but following that never become unready before its livenessProbe fail and
    # restarts it if needed. To become unready following startup serves no
    # purpose as there are no other pod to fallback to in our non-HA deployment.
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 2
    failureThreshold: 1000
    timeoutSeconds: 1
  existingSecret:
  serviceAccount:
    annotations: { }
  extraPodSpec: { }

rbac:
  enabled: true

# proxy relates to the proxy pod, the proxy-public service, and the autohttps
# pod and proxy-http service.
proxy:
  secretToken:
  annotations: { }
  deploymentStrategy:
    ## type: Recreate
    ## - JupyterHub's interaction with the CHP proxy becomes a lot more robust
    ##   with this configuration. To understand this, consider that JupyterHub
    ##   during startup will interact a lot with the k8s service to reach a
    ##   ready proxy pod. If the hub pod during a helm upgrade is restarting
    ##   directly while the proxy pod is making a rolling upgrade, the hub pod
    ##   could end up running a sequence of interactions with the old proxy pod
    ##   and finishing up the sequence of interactions with the new proxy pod.
    ##   As CHP proxy pods carry individual state this is very error prone. One
    ##   outcome when not using Recreate as a strategy has been that user pods
    ##   have been deleted by the hub pod because it considered them unreachable
    ##   as it only configured the old proxy pod but not the new before trying
    ##   to reach them.
    type: Recreate
    ## rollingUpdate:
    ## - WARNING:
    ##   This is required to be set explicitly blank! Without it being
    ##   explicitly blank, k8s will let eventual old values under rollingUpdate
    ##   remain and then the Deployment becomes invalid and a helm upgrade would
    ##   fail with an error like this:
    ##
    ##     UPGRADE FAILED
    ##     Error: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
    ##     Error: UPGRADE FAILED: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
    rollingUpdate:
  # service relates to the proxy-public service
  service:
    type: ClusterIP
    labels: { }
    annotations: { }
    nodePorts:
      http:
      https:
    disableHttpPort: false
    extraPorts: [ ]
    loadBalancerIP:
    loadBalancerSourceRanges: [ ]
  # chp relates to the proxy pod, which is responsible for routing traffic based
  # on dynamic configuration sent from JupyterHub to CHP's REST API.
  chp:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: jupyterhub/configurable-http-proxy
      tag: 4.5.0 # https://github.com/jupyterhub/configurable-http-proxy/releases
      pullPolicy:
      pullSecrets: [ ]
    extraCommandLineFlags: [ ]
    livenessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 10
    readinessProbe:
      enabled: true
      initialDelaySeconds: 0
      periodSeconds: 2
      failureThreshold: 1000
    resources: { }
    defaultTarget:
    errorTarget:
    extraEnv: { }
    nodeSelector: { }
    tolerations: [ ]
    networkPolicy:
      enabled: true
      ingress: [ ]
      egress:
        - to:
            - ipBlock:
                cidr: 0.0.0.0/0
      interNamespaceAccessLabels: ignore
      allowedIngressPorts: [ http, https ]
    pdb:
      enabled: false
      maxUnavailable:
      minAvailable: 1
    extraPodSpec: { }
  # traefik relates to the autohttps pod, which is responsible for TLS
  # termination when proxy.https.type=letsencrypt.
  traefik:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: traefik
      tag: v2.4.11 # ref: https://hub.docker.com/_/traefik?tab=tags
      pullPolicy:
      pullSecrets: [ ]
    hsts:
      includeSubdomains: false
      preload: false
      maxAge: 15724800 # About 6 months
    resources: { }
    labels: { }
    extraEnv: { }
    extraVolumes: [ ]
    extraVolumeMounts: [ ]
    extraStaticConfig: { }
    extraDynamicConfig: { }
    nodeSelector: { }
    tolerations: [ ]
    extraPorts: [ ]
    networkPolicy:
      enabled: true
      ingress: [ ]
      egress:
        - to:
            - ipBlock:
                cidr: 0.0.0.0/0
      interNamespaceAccessLabels: ignore
      allowedIngressPorts: [ http, https ]
    pdb:
      enabled: false
      maxUnavailable:
      minAvailable: 1
    serviceAccount:
      annotations: { }
    extraPodSpec: { }
  secretSync:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: jupyterhub/k8s-secret-sync
      tag: "1.2.0"
      pullPolicy:
      pullSecrets: [ ]
    resources: { }
  labels: { }
  https:
    enabled: false
    type: letsencrypt
    #type: letsencrypt, manual, offload, secret
    letsencrypt:
      contactEmail:
      # Specify custom server here (https://acme-staging-v02.api.letsencrypt.org/directory) to hit staging LE
      acmeServer: https://acme-v02.api.letsencrypt.org/directory
    manual:
      key:
      cert:
    secret:
      name:
      key: tls.key
      crt: tls.crt
    hosts: [ ]

# singleuser relates to the configuration of KubeSpawner which runs in the hub
# pod, and its spawning of user pods such as jupyter-myusername.
singleuser:
  podNameTemplate:
  extraTolerations: [ ]
  nodeSelector: { }
  extraNodeAffinity:
    required: [ ]
    preferred: [ ]
  extraPodAffinity:
    required: [ ]
    preferred: [ ]
  extraPodAntiAffinity:
    required: [ ]
    preferred: [ ]
  networkTools:
    image:
      name: jupyterhub/k8s-network-tools
      tag: "1.2.0"
      pullPolicy:
      pullSecrets: [ ]
  cloudMetadata:
    # block set to true will append a privileged initContainer using the
    # iptables to block the sensitive metadata server at the provided ip.
    blockWithIptables: false
    ip: 169.254.169.254
  networkPolicy:
    enabled: true
    ingress: [ ]
    egress:
      # Required egress to communicate with the hub and DNS servers will be
      # augmented to these egress rules.
      #
      # This default rule explicitly allows all outbound traffic from singleuser
      # pods, except to a typical IP used to return metadata that can be used by
      # someone with malicious intent.
      - to:
          - ipBlock:
              cidr: 0.0.0.0/0
              except:
                - 169.254.169.254/32
    interNamespaceAccessLabels: ignore
    allowedIngressPorts: [ ]
  events: true
  extraAnnotations: { }
  extraLabels:
    hub.jupyter.org/network-access-hub: "true"
  extraFiles: { }
  extraEnv:
    CHOWN_HOME: "yes"
    JUPYTER_ENABLE_LAB: "yes"
    LIBERATOR_REST_HOST: 10.12.32.22
    LIBERATOR_REST_PORT: "47753"
    LIBERATOR_SYSTEM: CQ.AI
  lifecycleHooks:
    postStart:
      exec:
        command:
          - bash
          - -c
          - |
            set -e ; : 'warning: -e is inactive in many constructs' ;
            echo jovyan:x:1000: >> /etc/group ;
            echo ${NB_USER}:x:${NB_UID}:1000::/home/jovyan:/bin/bash >> /etc/passwd;
            echo ${NB_USER}:x:${NB_UID}: >> /etc/group;
            # LDAP_GROUPS: CQ.AI only, in hub/extraConfig/supplementalGidsHook
            if [ -n "$LDAP_GROUPS" ] ; then
              # can't use groupadd, groupmems, usermod.  sigh.
              groups=   # will be comma-delimited group names
              for group_gid in ${LDAP_GROUPS//:/ } ; do
                group=${group_gid%=*}
                gid=${group_gid##*=}
                if [ -n "$group" ] && [ -n "$gid" ] ; then
                  grep -qs "$group:" /etc/group || {
                    # won't work if LDAP group name same as 1 in /etc/group
                    echo "$group:x:$gid:$NB_USER" >> /etc/group
                  }
                fi
              done
            fi
            ln -s jovyan /home/${NB_USER} ||: ;
            mkdir -p cqai/market-data cqai/data cqai/showcase;
            mkdir -p custom-env/envs custom-env/pkgs;
            ln -sf /srv/cqai/data/* cqai/data/ ||: ;
            ln -sf /srv/cqai/data/mktdb5/* cqai/market-data/ ||: ;
            cp /base/custom-env/* custom-env;
            ln -sf /srv/shared-scripts cqai/ ||: ;
            ln -sf /srv/showcase cqai/ ||: ;
            ln -sf /srv/tutorials cqai/ ||: ;
            ln -sf /srv/admin cqai/ ||: ;
            ln -sf /srv/team-env custom-env/ ||: ;
            mkdir -p /home/jovyan/.local/share/jupyter/snippets;
            rm -rf /home/jovyan/.local/share/jupyter/snippets;
            cp -r --preserve=mode /base/snippets /home/jovyan/.local/share/jupyter/.;
            cp /opt/conda/etc/jupyter/jupyter_notebook_config.py /home/jovyan/.jupyter/ ||: ;
  initContainers: [ ]
  extraContainers: [ ]
  uid: 1000
  fsGid: 100
  serviceAccountName: ""
  storage:
    type: dynamic
    extraLabels: { }
    extraVolumes:
      - name: cqai-data-volume
        persistentVolumeClaim:
          claimName: cqai-data-pvc
      - name: cqai-bashrc-profile-volume
        persistentVolumeClaim:
          claimName: cqai-bashrc-profile-pvc
    extraVolumeMounts:
    - mountPath: /var/data/shared/profile
      name: cqai-bashrc-profile-volume
      readOnly: true
    - mountPath: /srv/cqai/data
      name: cqai-data-volume
      readOnly: true
      subPath: data
    - mountPath: /srv/cqai/showcase
      name: cqai-data-volume
      readOnly: false
      subPath: showcase
    - mountPath: /srv/mktdb5
      name: cqai-data-volume
      readOnly: true
      subPath: data/mktdb5
    - mountPath: /srv/shared-scripts
      name: cqai-data-volume
      readOnly: false
      subPath: shared-scripts
    - mountPath: /srv/tutorials
      name: cqai-data-volume
      readOnly: false
      subPath: tutorials
    - mountPath: /srv/team-env
      name: cqai-data-volume
      readOnly: false
      subPath: conda-env
    - mountPath: /srv/admin
      name: cqai-data-volume
      readOnly: false
      subPath: admin
    static:
      pvcName: cqai-userdata-pvc
      subPath: home/{username}
    capacity: 10Gi
    homeMountPath: /home/jovyan
    dynamic:
      storageClass:
      pvcNameTemplate: claim-{username}{servername}
      volumeNameTemplate: volume-{username}{servername}
      storageAccessModes: [ ReadWriteOnce ]
  image:
    name: docker.cloudquant.ai/cqai-spoke
    tag: "0.1.93"
    pullPolicy:
    pullSecrets: [ ]
  startTimeout: 300
  cpu:
    limit:
    guarantee:
  memory:
    limit:
    guarantee: 1G
  extraResource:
    limits: { }
    guarantees: { }
  cmd: jupyterhub-singleuser
  defaultUrl: /lab
  extraPodConfig: { }
  profileList: [ ]

# scheduling relates to the user-scheduler pods and user-placeholder pods.
scheduling:
  userScheduler:
    enabled: false
    replicas: 2
    logLevel: 4
    # plugins ref: https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins-1
    plugins:
      score:
        disabled:
          - name: SelectorSpread
          - name: TaintToleration
          - name: PodTopologySpread
          - name: NodeResourcesBalancedAllocation
          - name: NodeResourcesLeastAllocated
          # Disable plugins to be allowed to enable them again with a different
          # weight and avoid an error.
          - name: NodePreferAvoidPods
          - name: NodeAffinity
          - name: InterPodAffinity
          - name: ImageLocality
        enabled:
          - name: NodePreferAvoidPods
            weight: 161051
          - name: NodeAffinity
            weight: 14631
          - name: InterPodAffinity
            weight: 1331
          - name: NodeResourcesMostAllocated
            weight: 121
          - name: ImageLocality
            weight: 11
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      # IMPORTANT: Bumping the minor version of this binary should go hand in
      #            hand with an inspection of the user-scheduelrs RBAC resources
      #            that we have forked.
      name: k8s.gcr.io/kube-scheduler
      tag: v1.19.13 # ref: https://github.com/kubernetes/website/blob/main/content/en/releases/patch-releases.md
      pullPolicy:
      pullSecrets: [ ]
    nodeSelector: { }
    tolerations: [ ]
    pdb:
      enabled: true
      maxUnavailable: 1
      minAvailable:
    resources: { }
    serviceAccount:
      annotations: { }
    extraPodSpec: { }
  podPriority:
    enabled: false
    globalDefault: false
    defaultPriority: 0
    userPlaceholderPriority: -10
  userPlaceholder:
    enabled: false
    image:
      name: k8s.gcr.io/pause
      # tag's can be updated by inspecting the output of the command:
      # gcloud container images list-tags k8s.gcr.io/pause --sort-by=~tags
      #
      # If you update this, also update prePuller.pause.image.tag
      tag: "3.5"
      pullPolicy:
      pullSecrets: [ ]
    replicas: 0
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    resources: { }
  corePods:
    tolerations:
      - key: hub.jupyter.org/dedicated
        operator: Equal
        value: core
        effect: NoSchedule
      - key: hub.jupyter.org_dedicated
        operator: Equal
        value: core
        effect: NoSchedule
    nodeAffinity:
      matchNodePurpose: prefer
  userPods:
    tolerations:
      - key: hub.jupyter.org/dedicated
        operator: Equal
        value: user
        effect: NoSchedule
      - key: hub.jupyter.org_dedicated
        operator: Equal
        value: user
        effect: NoSchedule
    nodeAffinity:
      matchNodePurpose: prefer

# prePuller relates to the hook|continuous-image-puller DaemonsSets
prePuller:
  annotations: { }
  resources: { }
  containerSecurityContext:
    runAsUser: 65534 # nobody user
    runAsGroup: 65534 # nobody group
    allowPrivilegeEscalation: false
  extraTolerations: [ ]
  # hook relates to the hook-image-awaiter Job and hook-image-puller DaemonSet
  hook:
    enabled: false
    pullOnlyOnChanges: true
    # image and the configuration below relates to the hook-image-awaiter Job
    image:
      name: jupyterhub/k8s-image-awaiter
      tag: "1.2.0"
      pullPolicy:
      pullSecrets: [ ]
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    podSchedulingWaitDuration: 10
    nodeSelector: { }
    tolerations: [ ]
    resources: { }
    serviceAccount:
      annotations: { }
  continuous:
    enabled: false
  pullProfileListImages: true
  extraImages: { }
  pause:
    containerSecurityContext:
      runAsUser: 65534 # nobody user
      runAsGroup: 65534 # nobody group
      allowPrivilegeEscalation: false
    image:
      name: k8s.gcr.io/pause
      # tag's can be updated by inspecting the output of the command:
      # gcloud container images list-tags k8s.gcr.io/pause --sort-by=~tags
      #
      # If you update this, also update scheduling.userPlaceholder.image.tag
      tag: "3.5"
      pullPolicy:
      pullSecrets: [ ]

ingress:
  enabled: true
  public: false
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production-dns
  hosts:
    - dkuleshov-hub.dev.cloudquant.ai
  pathSuffix:
  pathType: Prefix
  tls:
    - hosts:
        - dkuleshov-hub.dev.cloudquant.ai
      secretName: dkuleshov-hub.dev.cloudquant.ai-tls

# cull relates to the jupyterhub-idle-culler service, responsible for evicting
# inactive singleuser pods.
#
# The configuration below, except for enabled, corresponds to command-line flags
# for jupyterhub-idle-culler as documented here:
# https://github.com/jupyterhub/jupyterhub-idle-culler#as-a-standalone-script
#
cull:
  enabled: true
  users: false # --cull-users
  removeNamedServers: false # --remove-named-servers
  timeout: 86400 # --timeout
  every: 3600 # --cull-every
  concurrency: 10 # --concurrency
  maxAge: 0 # --max-age

debug:
  enabled: true

global:
  safeToShowValues: false
